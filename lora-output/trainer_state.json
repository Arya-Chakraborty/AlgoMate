{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.010123531184091594,
  "eval_steps": 500,
  "global_step": 4200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.4103645676408555e-05,
      "grad_norm": 19.568359375,
      "learning_rate": 4.999963844531485e-06,
      "loss": 3.2622,
      "step": 10
    },
    {
      "epoch": 4.820729135281711e-05,
      "grad_norm": 31.454788208007812,
      "learning_rate": 4.999923671788692e-06,
      "loss": 3.3999,
      "step": 20
    },
    {
      "epoch": 7.231093702922567e-05,
      "grad_norm": 36.64789581298828,
      "learning_rate": 4.999883499045898e-06,
      "loss": 3.6095,
      "step": 30
    },
    {
      "epoch": 9.641458270563422e-05,
      "grad_norm": 26.007898330688477,
      "learning_rate": 4.999843326303104e-06,
      "loss": 3.1143,
      "step": 40
    },
    {
      "epoch": 0.00012051822838204278,
      "grad_norm": 27.7662353515625,
      "learning_rate": 4.99980315356031e-06,
      "loss": 2.6678,
      "step": 50
    },
    {
      "epoch": 0.00014462187405845135,
      "grad_norm": 35.028770446777344,
      "learning_rate": 4.9997629808175155e-06,
      "loss": 3.0221,
      "step": 60
    },
    {
      "epoch": 0.0001687255197348599,
      "grad_norm": 23.8351993560791,
      "learning_rate": 4.999722808074722e-06,
      "loss": 3.1154,
      "step": 70
    },
    {
      "epoch": 0.00019282916541126844,
      "grad_norm": 31.910602569580078,
      "learning_rate": 4.999682635331928e-06,
      "loss": 2.725,
      "step": 80
    },
    {
      "epoch": 0.000216932811087677,
      "grad_norm": 17.973186492919922,
      "learning_rate": 4.999642462589134e-06,
      "loss": 3.0642,
      "step": 90
    },
    {
      "epoch": 0.00024103645676408557,
      "grad_norm": 11.413559913635254,
      "learning_rate": 4.99960228984634e-06,
      "loss": 3.0566,
      "step": 100
    },
    {
      "epoch": 0.0002651401024404941,
      "grad_norm": 24.9272403717041,
      "learning_rate": 4.999562117103546e-06,
      "loss": 3.1397,
      "step": 110
    },
    {
      "epoch": 0.0002892437481169027,
      "grad_norm": 18.51283836364746,
      "learning_rate": 4.9995219443607515e-06,
      "loss": 2.9584,
      "step": 120
    },
    {
      "epoch": 0.0003133473937933112,
      "grad_norm": 29.334548950195312,
      "learning_rate": 4.999481771617958e-06,
      "loss": 2.9374,
      "step": 130
    },
    {
      "epoch": 0.0003374510394697198,
      "grad_norm": 26.962440490722656,
      "learning_rate": 4.999441598875164e-06,
      "loss": 2.629,
      "step": 140
    },
    {
      "epoch": 0.00036155468514612835,
      "grad_norm": 18.1955623626709,
      "learning_rate": 4.99940142613237e-06,
      "loss": 2.857,
      "step": 150
    },
    {
      "epoch": 0.0003856583308225369,
      "grad_norm": 18.418292999267578,
      "learning_rate": 4.999361253389576e-06,
      "loss": 2.6552,
      "step": 160
    },
    {
      "epoch": 0.0004097619764989455,
      "grad_norm": 32.40252685546875,
      "learning_rate": 4.999321080646782e-06,
      "loss": 2.6439,
      "step": 170
    },
    {
      "epoch": 0.000433865622175354,
      "grad_norm": 21.132854461669922,
      "learning_rate": 4.9992809079039875e-06,
      "loss": 2.6939,
      "step": 180
    },
    {
      "epoch": 0.0004579692678517626,
      "grad_norm": 25.678682327270508,
      "learning_rate": 4.999240735161194e-06,
      "loss": 2.5593,
      "step": 190
    },
    {
      "epoch": 0.00048207291352817113,
      "grad_norm": 22.893314361572266,
      "learning_rate": 4.9992005624184e-06,
      "loss": 2.3661,
      "step": 200
    },
    {
      "epoch": 0.0005061765592045797,
      "grad_norm": 20.01263427734375,
      "learning_rate": 4.999160389675605e-06,
      "loss": 2.8436,
      "step": 210
    },
    {
      "epoch": 0.0005302802048809882,
      "grad_norm": 19.259424209594727,
      "learning_rate": 4.999120216932811e-06,
      "loss": 2.8521,
      "step": 220
    },
    {
      "epoch": 0.0005543838505573968,
      "grad_norm": 13.970054626464844,
      "learning_rate": 4.999080044190018e-06,
      "loss": 2.9357,
      "step": 230
    },
    {
      "epoch": 0.0005784874962338054,
      "grad_norm": 16.408159255981445,
      "learning_rate": 4.9990398714472235e-06,
      "loss": 2.5303,
      "step": 240
    },
    {
      "epoch": 0.0006025911419102139,
      "grad_norm": 35.232078552246094,
      "learning_rate": 4.998999698704429e-06,
      "loss": 2.3391,
      "step": 250
    },
    {
      "epoch": 0.0006266947875866225,
      "grad_norm": 14.597718238830566,
      "learning_rate": 4.998959525961635e-06,
      "loss": 3.0639,
      "step": 260
    },
    {
      "epoch": 0.000650798433263031,
      "grad_norm": 19.776569366455078,
      "learning_rate": 4.998919353218841e-06,
      "loss": 2.7611,
      "step": 270
    },
    {
      "epoch": 0.0006749020789394396,
      "grad_norm": 15.377687454223633,
      "learning_rate": 4.998879180476047e-06,
      "loss": 2.6117,
      "step": 280
    },
    {
      "epoch": 0.0006990057246158482,
      "grad_norm": 13.089600563049316,
      "learning_rate": 4.998839007733254e-06,
      "loss": 2.4246,
      "step": 290
    },
    {
      "epoch": 0.0007231093702922567,
      "grad_norm": 29.693161010742188,
      "learning_rate": 4.9987988349904595e-06,
      "loss": 2.751,
      "step": 300
    },
    {
      "epoch": 0.0007472130159686652,
      "grad_norm": 20.983245849609375,
      "learning_rate": 4.998758662247665e-06,
      "loss": 2.1894,
      "step": 310
    },
    {
      "epoch": 0.0007713166616450738,
      "grad_norm": 18.04589080810547,
      "learning_rate": 4.998718489504871e-06,
      "loss": 3.2773,
      "step": 320
    },
    {
      "epoch": 0.0007954203073214824,
      "grad_norm": 20.01802635192871,
      "learning_rate": 4.998678316762077e-06,
      "loss": 2.8451,
      "step": 330
    },
    {
      "epoch": 0.000819523952997891,
      "grad_norm": 22.546382904052734,
      "learning_rate": 4.998638144019283e-06,
      "loss": 2.5843,
      "step": 340
    },
    {
      "epoch": 0.0008436275986742995,
      "grad_norm": 27.923826217651367,
      "learning_rate": 4.99859797127649e-06,
      "loss": 2.453,
      "step": 350
    },
    {
      "epoch": 0.000867731244350708,
      "grad_norm": 18.057849884033203,
      "learning_rate": 4.9985577985336954e-06,
      "loss": 2.3275,
      "step": 360
    },
    {
      "epoch": 0.0008918348900271166,
      "grad_norm": 14.919589042663574,
      "learning_rate": 4.998517625790901e-06,
      "loss": 2.465,
      "step": 370
    },
    {
      "epoch": 0.0009159385357035252,
      "grad_norm": 23.870248794555664,
      "learning_rate": 4.998477453048107e-06,
      "loss": 2.6204,
      "step": 380
    },
    {
      "epoch": 0.0009400421813799337,
      "grad_norm": 22.811798095703125,
      "learning_rate": 4.998437280305313e-06,
      "loss": 2.6988,
      "step": 390
    },
    {
      "epoch": 0.0009641458270563423,
      "grad_norm": 21.942777633666992,
      "learning_rate": 4.998397107562519e-06,
      "loss": 2.4041,
      "step": 400
    },
    {
      "epoch": 0.0009882494727327508,
      "grad_norm": 18.20435905456543,
      "learning_rate": 4.998356934819726e-06,
      "loss": 2.1754,
      "step": 410
    },
    {
      "epoch": 0.0010123531184091593,
      "grad_norm": 21.8759765625,
      "learning_rate": 4.9983167620769314e-06,
      "loss": 3.0083,
      "step": 420
    },
    {
      "epoch": 0.0010364567640855679,
      "grad_norm": 24.735509872436523,
      "learning_rate": 4.998276589334137e-06,
      "loss": 2.9808,
      "step": 430
    },
    {
      "epoch": 0.0010605604097619764,
      "grad_norm": 22.009244918823242,
      "learning_rate": 4.998236416591343e-06,
      "loss": 2.3788,
      "step": 440
    },
    {
      "epoch": 0.0010846640554383852,
      "grad_norm": 20.31003761291504,
      "learning_rate": 4.998196243848549e-06,
      "loss": 2.5981,
      "step": 450
    },
    {
      "epoch": 0.0011087677011147937,
      "grad_norm": 21.0109806060791,
      "learning_rate": 4.998156071105755e-06,
      "loss": 2.5298,
      "step": 460
    },
    {
      "epoch": 0.0011328713467912022,
      "grad_norm": 13.321985244750977,
      "learning_rate": 4.998115898362962e-06,
      "loss": 2.1023,
      "step": 470
    },
    {
      "epoch": 0.0011569749924676108,
      "grad_norm": 18.466293334960938,
      "learning_rate": 4.9980757256201674e-06,
      "loss": 2.6468,
      "step": 480
    },
    {
      "epoch": 0.0011810786381440193,
      "grad_norm": 16.59696388244629,
      "learning_rate": 4.998035552877373e-06,
      "loss": 2.4884,
      "step": 490
    },
    {
      "epoch": 0.0012051822838204278,
      "grad_norm": 19.66241455078125,
      "learning_rate": 4.997995380134579e-06,
      "loss": 2.4723,
      "step": 500
    },
    {
      "epoch": 0.0012292859294968364,
      "grad_norm": 19.76795768737793,
      "learning_rate": 4.997955207391785e-06,
      "loss": 2.0579,
      "step": 510
    },
    {
      "epoch": 0.001253389575173245,
      "grad_norm": 27.274202346801758,
      "learning_rate": 4.997915034648992e-06,
      "loss": 2.3649,
      "step": 520
    },
    {
      "epoch": 0.0012774932208496534,
      "grad_norm": 19.938215255737305,
      "learning_rate": 4.9978748619061976e-06,
      "loss": 2.7306,
      "step": 530
    },
    {
      "epoch": 0.001301596866526062,
      "grad_norm": 27.053754806518555,
      "learning_rate": 4.9978346891634034e-06,
      "loss": 1.9779,
      "step": 540
    },
    {
      "epoch": 0.0013257005122024707,
      "grad_norm": 13.685882568359375,
      "learning_rate": 4.9977945164206084e-06,
      "loss": 2.6871,
      "step": 550
    },
    {
      "epoch": 0.0013498041578788793,
      "grad_norm": 21.418039321899414,
      "learning_rate": 4.997754343677815e-06,
      "loss": 2.6605,
      "step": 560
    },
    {
      "epoch": 0.0013739078035552878,
      "grad_norm": 16.756168365478516,
      "learning_rate": 4.997714170935021e-06,
      "loss": 2.8827,
      "step": 570
    },
    {
      "epoch": 0.0013980114492316963,
      "grad_norm": 16.603200912475586,
      "learning_rate": 4.997673998192227e-06,
      "loss": 2.2409,
      "step": 580
    },
    {
      "epoch": 0.0014221150949081049,
      "grad_norm": 18.154966354370117,
      "learning_rate": 4.997633825449433e-06,
      "loss": 2.4137,
      "step": 590
    },
    {
      "epoch": 0.0014462187405845134,
      "grad_norm": 25.011688232421875,
      "learning_rate": 4.997593652706639e-06,
      "loss": 2.3725,
      "step": 600
    },
    {
      "epoch": 0.001470322386260922,
      "grad_norm": 18.655941009521484,
      "learning_rate": 4.9975534799638444e-06,
      "loss": 2.538,
      "step": 610
    },
    {
      "epoch": 0.0014944260319373305,
      "grad_norm": 16.826061248779297,
      "learning_rate": 4.997513307221051e-06,
      "loss": 1.9377,
      "step": 620
    },
    {
      "epoch": 0.001518529677613739,
      "grad_norm": 20.709753036499023,
      "learning_rate": 4.997473134478257e-06,
      "loss": 2.7854,
      "step": 630
    },
    {
      "epoch": 0.0015426333232901475,
      "grad_norm": 17.106555938720703,
      "learning_rate": 4.997432961735463e-06,
      "loss": 2.4435,
      "step": 640
    },
    {
      "epoch": 0.0015667369689665563,
      "grad_norm": 20.22747802734375,
      "learning_rate": 4.997392788992669e-06,
      "loss": 2.3309,
      "step": 650
    },
    {
      "epoch": 0.0015908406146429648,
      "grad_norm": 27.16120147705078,
      "learning_rate": 4.9973526162498746e-06,
      "loss": 2.4952,
      "step": 660
    },
    {
      "epoch": 0.0016149442603193734,
      "grad_norm": 5.105584144592285,
      "learning_rate": 4.9973124435070804e-06,
      "loss": 1.9621,
      "step": 670
    },
    {
      "epoch": 0.001639047905995782,
      "grad_norm": 15.712339401245117,
      "learning_rate": 4.997272270764287e-06,
      "loss": 2.2103,
      "step": 680
    },
    {
      "epoch": 0.0016631515516721904,
      "grad_norm": 16.09283447265625,
      "learning_rate": 4.997232098021493e-06,
      "loss": 2.0167,
      "step": 690
    },
    {
      "epoch": 0.001687255197348599,
      "grad_norm": 26.563922882080078,
      "learning_rate": 4.997191925278699e-06,
      "loss": 2.7393,
      "step": 700
    },
    {
      "epoch": 0.0017113588430250075,
      "grad_norm": 22.146038055419922,
      "learning_rate": 4.997151752535905e-06,
      "loss": 2.3653,
      "step": 710
    },
    {
      "epoch": 0.001735462488701416,
      "grad_norm": 15.818408966064453,
      "learning_rate": 4.9971115797931106e-06,
      "loss": 2.4657,
      "step": 720
    },
    {
      "epoch": 0.0017595661343778246,
      "grad_norm": 15.728766441345215,
      "learning_rate": 4.9970714070503164e-06,
      "loss": 2.8504,
      "step": 730
    },
    {
      "epoch": 0.0017836697800542331,
      "grad_norm": 16.15811538696289,
      "learning_rate": 4.997031234307523e-06,
      "loss": 2.1589,
      "step": 740
    },
    {
      "epoch": 0.0018077734257306419,
      "grad_norm": 21.293033599853516,
      "learning_rate": 4.996991061564729e-06,
      "loss": 3.365,
      "step": 750
    },
    {
      "epoch": 0.0018318770714070504,
      "grad_norm": 22.782854080200195,
      "learning_rate": 4.996950888821935e-06,
      "loss": 2.4926,
      "step": 760
    },
    {
      "epoch": 0.001855980717083459,
      "grad_norm": 14.787109375,
      "learning_rate": 4.996910716079141e-06,
      "loss": 1.7396,
      "step": 770
    },
    {
      "epoch": 0.0018800843627598675,
      "grad_norm": 15.30887508392334,
      "learning_rate": 4.9968705433363466e-06,
      "loss": 2.0883,
      "step": 780
    },
    {
      "epoch": 0.001904188008436276,
      "grad_norm": 33.725040435791016,
      "learning_rate": 4.996830370593552e-06,
      "loss": 2.6233,
      "step": 790
    },
    {
      "epoch": 0.0019282916541126845,
      "grad_norm": 14.57259750366211,
      "learning_rate": 4.996790197850759e-06,
      "loss": 2.4462,
      "step": 800
    },
    {
      "epoch": 0.001952395299789093,
      "grad_norm": 15.735634803771973,
      "learning_rate": 4.996750025107965e-06,
      "loss": 3.3058,
      "step": 810
    },
    {
      "epoch": 0.0019764989454655016,
      "grad_norm": 16.840778350830078,
      "learning_rate": 4.996709852365171e-06,
      "loss": 2.4461,
      "step": 820
    },
    {
      "epoch": 0.0020006025911419104,
      "grad_norm": 21.375743865966797,
      "learning_rate": 4.996669679622377e-06,
      "loss": 2.2001,
      "step": 830
    },
    {
      "epoch": 0.0020247062368183187,
      "grad_norm": 44.22355651855469,
      "learning_rate": 4.9966295068795826e-06,
      "loss": 2.2152,
      "step": 840
    },
    {
      "epoch": 0.0020488098824947274,
      "grad_norm": 17.368555068969727,
      "learning_rate": 4.996589334136788e-06,
      "loss": 2.2789,
      "step": 850
    },
    {
      "epoch": 0.0020729135281711357,
      "grad_norm": 22.848665237426758,
      "learning_rate": 4.996549161393995e-06,
      "loss": 2.859,
      "step": 860
    },
    {
      "epoch": 0.0020970171738475445,
      "grad_norm": 16.679319381713867,
      "learning_rate": 4.996508988651201e-06,
      "loss": 2.284,
      "step": 870
    },
    {
      "epoch": 0.002121120819523953,
      "grad_norm": 34.082794189453125,
      "learning_rate": 4.996468815908407e-06,
      "loss": 2.6333,
      "step": 880
    },
    {
      "epoch": 0.0021452244652003616,
      "grad_norm": 19.97458267211914,
      "learning_rate": 4.996428643165613e-06,
      "loss": 2.6198,
      "step": 890
    },
    {
      "epoch": 0.0021693281108767703,
      "grad_norm": 19.543254852294922,
      "learning_rate": 4.9963884704228185e-06,
      "loss": 2.7911,
      "step": 900
    },
    {
      "epoch": 0.0021934317565531786,
      "grad_norm": 31.410654067993164,
      "learning_rate": 4.996348297680024e-06,
      "loss": 2.6101,
      "step": 910
    },
    {
      "epoch": 0.0022175354022295874,
      "grad_norm": 19.700252532958984,
      "learning_rate": 4.99630812493723e-06,
      "loss": 2.6209,
      "step": 920
    },
    {
      "epoch": 0.0022416390479059957,
      "grad_norm": 16.13108253479004,
      "learning_rate": 4.996267952194436e-06,
      "loss": 2.2644,
      "step": 930
    },
    {
      "epoch": 0.0022657426935824045,
      "grad_norm": 14.939324378967285,
      "learning_rate": 4.996227779451642e-06,
      "loss": 2.3989,
      "step": 940
    },
    {
      "epoch": 0.0022898463392588128,
      "grad_norm": 19.233976364135742,
      "learning_rate": 4.996187606708849e-06,
      "loss": 2.3872,
      "step": 950
    },
    {
      "epoch": 0.0023139499849352215,
      "grad_norm": 13.392967224121094,
      "learning_rate": 4.9961474339660545e-06,
      "loss": 1.9681,
      "step": 960
    },
    {
      "epoch": 0.00233805363061163,
      "grad_norm": 18.498336791992188,
      "learning_rate": 4.99610726122326e-06,
      "loss": 2.1706,
      "step": 970
    },
    {
      "epoch": 0.0023621572762880386,
      "grad_norm": 25.52715301513672,
      "learning_rate": 4.996067088480466e-06,
      "loss": 2.4911,
      "step": 980
    },
    {
      "epoch": 0.002386260921964447,
      "grad_norm": 29.518890380859375,
      "learning_rate": 4.996026915737672e-06,
      "loss": 2.0628,
      "step": 990
    },
    {
      "epoch": 0.0024103645676408557,
      "grad_norm": 13.717429161071777,
      "learning_rate": 4.995986742994878e-06,
      "loss": 2.3028,
      "step": 1000
    },
    {
      "epoch": 0.0024344682133172644,
      "grad_norm": 25.039594650268555,
      "learning_rate": 4.995946570252085e-06,
      "loss": 1.9436,
      "step": 1010
    },
    {
      "epoch": 0.0024585718589936727,
      "grad_norm": 21.897220611572266,
      "learning_rate": 4.9959063975092905e-06,
      "loss": 2.1628,
      "step": 1020
    },
    {
      "epoch": 0.0024826755046700815,
      "grad_norm": 16.643272399902344,
      "learning_rate": 4.995866224766496e-06,
      "loss": 2.3808,
      "step": 1030
    },
    {
      "epoch": 0.00250677915034649,
      "grad_norm": 14.486332893371582,
      "learning_rate": 4.995826052023702e-06,
      "loss": 2.3503,
      "step": 1040
    },
    {
      "epoch": 0.0025308827960228986,
      "grad_norm": 17.296022415161133,
      "learning_rate": 4.995785879280908e-06,
      "loss": 2.6556,
      "step": 1050
    },
    {
      "epoch": 0.002554986441699307,
      "grad_norm": 21.88452911376953,
      "learning_rate": 4.995745706538114e-06,
      "loss": 2.5861,
      "step": 1060
    },
    {
      "epoch": 0.0025790900873757156,
      "grad_norm": 32.9426155090332,
      "learning_rate": 4.995705533795321e-06,
      "loss": 2.5331,
      "step": 1070
    },
    {
      "epoch": 0.002603193733052124,
      "grad_norm": 16.010175704956055,
      "learning_rate": 4.9956653610525265e-06,
      "loss": 2.5916,
      "step": 1080
    },
    {
      "epoch": 0.0026272973787285327,
      "grad_norm": 30.6175479888916,
      "learning_rate": 4.995625188309732e-06,
      "loss": 1.8518,
      "step": 1090
    },
    {
      "epoch": 0.0026514010244049415,
      "grad_norm": 13.405879974365234,
      "learning_rate": 4.995585015566938e-06,
      "loss": 2.3239,
      "step": 1100
    },
    {
      "epoch": 0.0026755046700813498,
      "grad_norm": 21.778711318969727,
      "learning_rate": 4.995544842824144e-06,
      "loss": 2.5045,
      "step": 1110
    },
    {
      "epoch": 0.0026996083157577585,
      "grad_norm": 21.15515899658203,
      "learning_rate": 4.99550467008135e-06,
      "loss": 2.7748,
      "step": 1120
    },
    {
      "epoch": 0.002723711961434167,
      "grad_norm": 11.243205070495605,
      "learning_rate": 4.995464497338557e-06,
      "loss": 1.6684,
      "step": 1130
    },
    {
      "epoch": 0.0027478156071105756,
      "grad_norm": 17.64056396484375,
      "learning_rate": 4.9954243245957625e-06,
      "loss": 2.1555,
      "step": 1140
    },
    {
      "epoch": 0.002771919252786984,
      "grad_norm": 25.766511917114258,
      "learning_rate": 4.995384151852968e-06,
      "loss": 3.0925,
      "step": 1150
    },
    {
      "epoch": 0.0027960228984633927,
      "grad_norm": 15.34792709350586,
      "learning_rate": 4.995343979110174e-06,
      "loss": 2.3186,
      "step": 1160
    },
    {
      "epoch": 0.002820126544139801,
      "grad_norm": 23.563220977783203,
      "learning_rate": 4.99530380636738e-06,
      "loss": 2.3156,
      "step": 1170
    },
    {
      "epoch": 0.0028442301898162097,
      "grad_norm": 18.597196578979492,
      "learning_rate": 4.995263633624586e-06,
      "loss": 2.4516,
      "step": 1180
    },
    {
      "epoch": 0.002868333835492618,
      "grad_norm": 4.0386552810668945,
      "learning_rate": 4.995223460881793e-06,
      "loss": 2.3834,
      "step": 1190
    },
    {
      "epoch": 0.002892437481169027,
      "grad_norm": 19.10190773010254,
      "learning_rate": 4.9951832881389985e-06,
      "loss": 2.2818,
      "step": 1200
    },
    {
      "epoch": 0.0029165411268454356,
      "grad_norm": 23.51725196838379,
      "learning_rate": 4.995143115396204e-06,
      "loss": 2.3724,
      "step": 1210
    },
    {
      "epoch": 0.002940644772521844,
      "grad_norm": 18.854843139648438,
      "learning_rate": 4.99510294265341e-06,
      "loss": 1.7785,
      "step": 1220
    },
    {
      "epoch": 0.0029647484181982526,
      "grad_norm": 33.478912353515625,
      "learning_rate": 4.995062769910616e-06,
      "loss": 2.3346,
      "step": 1230
    },
    {
      "epoch": 0.002988852063874661,
      "grad_norm": 14.775910377502441,
      "learning_rate": 4.995022597167822e-06,
      "loss": 2.0447,
      "step": 1240
    },
    {
      "epoch": 0.0030129557095510697,
      "grad_norm": 25.803430557250977,
      "learning_rate": 4.994982424425028e-06,
      "loss": 2.674,
      "step": 1250
    },
    {
      "epoch": 0.003037059355227478,
      "grad_norm": 20.42286491394043,
      "learning_rate": 4.994942251682234e-06,
      "loss": 2.2956,
      "step": 1260
    },
    {
      "epoch": 0.0030611630009038868,
      "grad_norm": 4.835489749908447,
      "learning_rate": 4.9949020789394395e-06,
      "loss": 1.5254,
      "step": 1270
    },
    {
      "epoch": 0.003085266646580295,
      "grad_norm": 21.028345108032227,
      "learning_rate": 4.994861906196645e-06,
      "loss": 2.2583,
      "step": 1280
    },
    {
      "epoch": 0.003109370292256704,
      "grad_norm": 12.703229904174805,
      "learning_rate": 4.994821733453852e-06,
      "loss": 2.0904,
      "step": 1290
    },
    {
      "epoch": 0.0031334739379331126,
      "grad_norm": 22.6090145111084,
      "learning_rate": 4.994781560711058e-06,
      "loss": 1.6815,
      "step": 1300
    },
    {
      "epoch": 0.003157577583609521,
      "grad_norm": 16.412567138671875,
      "learning_rate": 4.994741387968264e-06,
      "loss": 1.8698,
      "step": 1310
    },
    {
      "epoch": 0.0031816812292859297,
      "grad_norm": 24.4211483001709,
      "learning_rate": 4.99470121522547e-06,
      "loss": 2.7646,
      "step": 1320
    },
    {
      "epoch": 0.003205784874962338,
      "grad_norm": 5.894290447235107,
      "learning_rate": 4.9946610424826755e-06,
      "loss": 2.0737,
      "step": 1330
    },
    {
      "epoch": 0.0032298885206387467,
      "grad_norm": 28.326194763183594,
      "learning_rate": 4.994620869739882e-06,
      "loss": 2.4949,
      "step": 1340
    },
    {
      "epoch": 0.003253992166315155,
      "grad_norm": 16.936819076538086,
      "learning_rate": 4.994580696997088e-06,
      "loss": 1.9669,
      "step": 1350
    },
    {
      "epoch": 0.003278095811991564,
      "grad_norm": 13.257911682128906,
      "learning_rate": 4.994540524254294e-06,
      "loss": 2.2686,
      "step": 1360
    },
    {
      "epoch": 0.003302199457667972,
      "grad_norm": 9.276871681213379,
      "learning_rate": 4.9945003515115e-06,
      "loss": 2.2091,
      "step": 1370
    },
    {
      "epoch": 0.003326303103344381,
      "grad_norm": 22.758865356445312,
      "learning_rate": 4.994460178768706e-06,
      "loss": 1.8418,
      "step": 1380
    },
    {
      "epoch": 0.003350406749020789,
      "grad_norm": 14.65982723236084,
      "learning_rate": 4.9944200060259115e-06,
      "loss": 2.409,
      "step": 1390
    },
    {
      "epoch": 0.003374510394697198,
      "grad_norm": 17.884151458740234,
      "learning_rate": 4.994379833283118e-06,
      "loss": 2.7383,
      "step": 1400
    },
    {
      "epoch": 0.0033986140403736067,
      "grad_norm": 14.55929183959961,
      "learning_rate": 4.994339660540324e-06,
      "loss": 2.1726,
      "step": 1410
    },
    {
      "epoch": 0.003422717686050015,
      "grad_norm": 19.6298770904541,
      "learning_rate": 4.99429948779753e-06,
      "loss": 2.0683,
      "step": 1420
    },
    {
      "epoch": 0.0034468213317264238,
      "grad_norm": 22.352157592773438,
      "learning_rate": 4.994259315054736e-06,
      "loss": 1.7887,
      "step": 1430
    },
    {
      "epoch": 0.003470924977402832,
      "grad_norm": 12.654948234558105,
      "learning_rate": 4.994219142311942e-06,
      "loss": 2.6008,
      "step": 1440
    },
    {
      "epoch": 0.003495028623079241,
      "grad_norm": 24.90059471130371,
      "learning_rate": 4.9941789695691475e-06,
      "loss": 2.5341,
      "step": 1450
    },
    {
      "epoch": 0.003519132268755649,
      "grad_norm": 15.072452545166016,
      "learning_rate": 4.994138796826354e-06,
      "loss": 1.8936,
      "step": 1460
    },
    {
      "epoch": 0.003543235914432058,
      "grad_norm": 17.31846046447754,
      "learning_rate": 4.99409862408356e-06,
      "loss": 1.9478,
      "step": 1470
    },
    {
      "epoch": 0.0035673395601084662,
      "grad_norm": 16.65842056274414,
      "learning_rate": 4.994058451340766e-06,
      "loss": 2.5147,
      "step": 1480
    },
    {
      "epoch": 0.003591443205784875,
      "grad_norm": 8.393778800964355,
      "learning_rate": 4.994018278597972e-06,
      "loss": 2.0432,
      "step": 1490
    },
    {
      "epoch": 0.0036155468514612837,
      "grad_norm": 19.945903778076172,
      "learning_rate": 4.993978105855178e-06,
      "loss": 2.3866,
      "step": 1500
    },
    {
      "epoch": 0.003639650497137692,
      "grad_norm": 21.172693252563477,
      "learning_rate": 4.9939379331123835e-06,
      "loss": 2.6461,
      "step": 1510
    },
    {
      "epoch": 0.003663754142814101,
      "grad_norm": 23.979190826416016,
      "learning_rate": 4.99389776036959e-06,
      "loss": 3.0519,
      "step": 1520
    },
    {
      "epoch": 0.003687857788490509,
      "grad_norm": 29.686147689819336,
      "learning_rate": 4.993857587626796e-06,
      "loss": 1.826,
      "step": 1530
    },
    {
      "epoch": 0.003711961434166918,
      "grad_norm": 14.150551795959473,
      "learning_rate": 4.993817414884002e-06,
      "loss": 2.0637,
      "step": 1540
    },
    {
      "epoch": 0.003736065079843326,
      "grad_norm": 18.553022384643555,
      "learning_rate": 4.993777242141208e-06,
      "loss": 1.7222,
      "step": 1550
    },
    {
      "epoch": 0.003760168725519735,
      "grad_norm": 34.76444625854492,
      "learning_rate": 4.993737069398414e-06,
      "loss": 2.2427,
      "step": 1560
    },
    {
      "epoch": 0.0037842723711961433,
      "grad_norm": 27.460416793823242,
      "learning_rate": 4.9936968966556195e-06,
      "loss": 2.6994,
      "step": 1570
    },
    {
      "epoch": 0.003808376016872552,
      "grad_norm": 21.116605758666992,
      "learning_rate": 4.993656723912825e-06,
      "loss": 2.4394,
      "step": 1580
    },
    {
      "epoch": 0.0038324796625489603,
      "grad_norm": 23.932720184326172,
      "learning_rate": 4.993616551170031e-06,
      "loss": 2.4665,
      "step": 1590
    },
    {
      "epoch": 0.003856583308225369,
      "grad_norm": 20.773523330688477,
      "learning_rate": 4.993576378427237e-06,
      "loss": 2.3003,
      "step": 1600
    },
    {
      "epoch": 0.003880686953901778,
      "grad_norm": 21.83485221862793,
      "learning_rate": 4.993536205684443e-06,
      "loss": 2.0654,
      "step": 1610
    },
    {
      "epoch": 0.003904790599578186,
      "grad_norm": 21.97788429260254,
      "learning_rate": 4.99349603294165e-06,
      "loss": 1.8006,
      "step": 1620
    },
    {
      "epoch": 0.003928894245254595,
      "grad_norm": 23.410446166992188,
      "learning_rate": 4.9934558601988555e-06,
      "loss": 2.6101,
      "step": 1630
    },
    {
      "epoch": 0.003952997890931003,
      "grad_norm": 19.138023376464844,
      "learning_rate": 4.993415687456061e-06,
      "loss": 2.3209,
      "step": 1640
    },
    {
      "epoch": 0.0039771015366074115,
      "grad_norm": 17.307598114013672,
      "learning_rate": 4.993375514713267e-06,
      "loss": 2.1926,
      "step": 1650
    },
    {
      "epoch": 0.004001205182283821,
      "grad_norm": 20.4220027923584,
      "learning_rate": 4.993335341970473e-06,
      "loss": 2.659,
      "step": 1660
    },
    {
      "epoch": 0.004025308827960229,
      "grad_norm": 31.70857048034668,
      "learning_rate": 4.993295169227679e-06,
      "loss": 2.2391,
      "step": 1670
    },
    {
      "epoch": 0.004049412473636637,
      "grad_norm": 19.07416343688965,
      "learning_rate": 4.993254996484886e-06,
      "loss": 2.092,
      "step": 1680
    },
    {
      "epoch": 0.004073516119313046,
      "grad_norm": 21.54515838623047,
      "learning_rate": 4.9932148237420915e-06,
      "loss": 2.5302,
      "step": 1690
    },
    {
      "epoch": 0.004097619764989455,
      "grad_norm": 18.297679901123047,
      "learning_rate": 4.993174650999297e-06,
      "loss": 2.0421,
      "step": 1700
    },
    {
      "epoch": 0.004121723410665863,
      "grad_norm": 15.833846092224121,
      "learning_rate": 4.993134478256503e-06,
      "loss": 2.204,
      "step": 1710
    },
    {
      "epoch": 0.0041458270563422715,
      "grad_norm": 26.81772232055664,
      "learning_rate": 4.993094305513709e-06,
      "loss": 2.1229,
      "step": 1720
    },
    {
      "epoch": 0.004169930702018681,
      "grad_norm": 21.61138343811035,
      "learning_rate": 4.993054132770915e-06,
      "loss": 1.5013,
      "step": 1730
    },
    {
      "epoch": 0.004194034347695089,
      "grad_norm": 17.68808364868164,
      "learning_rate": 4.993013960028122e-06,
      "loss": 2.4388,
      "step": 1740
    },
    {
      "epoch": 0.004218137993371497,
      "grad_norm": 23.09783363342285,
      "learning_rate": 4.9929737872853275e-06,
      "loss": 1.6456,
      "step": 1750
    },
    {
      "epoch": 0.004242241639047906,
      "grad_norm": 45.054893493652344,
      "learning_rate": 4.992933614542533e-06,
      "loss": 2.1268,
      "step": 1760
    },
    {
      "epoch": 0.004266345284724315,
      "grad_norm": 14.956257820129395,
      "learning_rate": 4.992893441799739e-06,
      "loss": 2.4698,
      "step": 1770
    },
    {
      "epoch": 0.004290448930400723,
      "grad_norm": 23.235933303833008,
      "learning_rate": 4.992853269056945e-06,
      "loss": 2.6236,
      "step": 1780
    },
    {
      "epoch": 0.0043145525760771315,
      "grad_norm": 17.446666717529297,
      "learning_rate": 4.992813096314152e-06,
      "loss": 2.2885,
      "step": 1790
    },
    {
      "epoch": 0.004338656221753541,
      "grad_norm": 20.797168731689453,
      "learning_rate": 4.992772923571358e-06,
      "loss": 2.3458,
      "step": 1800
    },
    {
      "epoch": 0.004362759867429949,
      "grad_norm": 17.57171630859375,
      "learning_rate": 4.9927327508285635e-06,
      "loss": 2.0522,
      "step": 1810
    },
    {
      "epoch": 0.004386863513106357,
      "grad_norm": 27.941974639892578,
      "learning_rate": 4.992692578085769e-06,
      "loss": 2.7849,
      "step": 1820
    },
    {
      "epoch": 0.004410967158782766,
      "grad_norm": 19.520565032958984,
      "learning_rate": 4.992652405342975e-06,
      "loss": 2.3425,
      "step": 1830
    },
    {
      "epoch": 0.004435070804459175,
      "grad_norm": 17.90438461303711,
      "learning_rate": 4.992612232600181e-06,
      "loss": 2.3015,
      "step": 1840
    },
    {
      "epoch": 0.004459174450135583,
      "grad_norm": 16.982585906982422,
      "learning_rate": 4.992572059857388e-06,
      "loss": 2.6077,
      "step": 1850
    },
    {
      "epoch": 0.004483278095811991,
      "grad_norm": 17.096107482910156,
      "learning_rate": 4.992531887114594e-06,
      "loss": 2.4835,
      "step": 1860
    },
    {
      "epoch": 0.0045073817414884,
      "grad_norm": 26.592771530151367,
      "learning_rate": 4.9924917143717995e-06,
      "loss": 2.2831,
      "step": 1870
    },
    {
      "epoch": 0.004531485387164809,
      "grad_norm": 18.027027130126953,
      "learning_rate": 4.992451541629005e-06,
      "loss": 2.4747,
      "step": 1880
    },
    {
      "epoch": 0.004555589032841217,
      "grad_norm": 12.735462188720703,
      "learning_rate": 4.992411368886211e-06,
      "loss": 2.6225,
      "step": 1890
    },
    {
      "epoch": 0.0045796926785176256,
      "grad_norm": 20.698440551757812,
      "learning_rate": 4.992371196143417e-06,
      "loss": 2.1118,
      "step": 1900
    },
    {
      "epoch": 0.004603796324194035,
      "grad_norm": 17.101011276245117,
      "learning_rate": 4.992331023400623e-06,
      "loss": 2.5249,
      "step": 1910
    },
    {
      "epoch": 0.004627899969870443,
      "grad_norm": 22.41545867919922,
      "learning_rate": 4.992290850657829e-06,
      "loss": 2.4783,
      "step": 1920
    },
    {
      "epoch": 0.004652003615546851,
      "grad_norm": 23.4083251953125,
      "learning_rate": 4.992250677915035e-06,
      "loss": 1.574,
      "step": 1930
    },
    {
      "epoch": 0.00467610726122326,
      "grad_norm": 13.691497802734375,
      "learning_rate": 4.9922105051722405e-06,
      "loss": 2.0195,
      "step": 1940
    },
    {
      "epoch": 0.004700210906899669,
      "grad_norm": 19.465572357177734,
      "learning_rate": 4.992170332429447e-06,
      "loss": 2.5137,
      "step": 1950
    },
    {
      "epoch": 0.004724314552576077,
      "grad_norm": 17.192472457885742,
      "learning_rate": 4.992130159686653e-06,
      "loss": 2.135,
      "step": 1960
    },
    {
      "epoch": 0.0047484181982524855,
      "grad_norm": 22.757661819458008,
      "learning_rate": 4.992089986943859e-06,
      "loss": 1.9477,
      "step": 1970
    },
    {
      "epoch": 0.004772521843928894,
      "grad_norm": 29.03984832763672,
      "learning_rate": 4.992049814201065e-06,
      "loss": 2.4608,
      "step": 1980
    },
    {
      "epoch": 0.004796625489605303,
      "grad_norm": 22.329544067382812,
      "learning_rate": 4.992009641458271e-06,
      "loss": 2.4478,
      "step": 1990
    },
    {
      "epoch": 0.004820729135281711,
      "grad_norm": 11.583111763000488,
      "learning_rate": 4.9919694687154765e-06,
      "loss": 2.6841,
      "step": 2000
    },
    {
      "epoch": 0.00484483278095812,
      "grad_norm": 33.40346145629883,
      "learning_rate": 4.991929295972683e-06,
      "loss": 2.2542,
      "step": 2010
    },
    {
      "epoch": 0.004868936426634529,
      "grad_norm": 26.461227416992188,
      "learning_rate": 4.991889123229889e-06,
      "loss": 2.3756,
      "step": 2020
    },
    {
      "epoch": 0.004893040072310937,
      "grad_norm": 21.365196228027344,
      "learning_rate": 4.991848950487095e-06,
      "loss": 1.9496,
      "step": 2030
    },
    {
      "epoch": 0.0049171437179873455,
      "grad_norm": 18.354896545410156,
      "learning_rate": 4.991808777744301e-06,
      "loss": 2.0761,
      "step": 2040
    },
    {
      "epoch": 0.004941247363663754,
      "grad_norm": 17.282695770263672,
      "learning_rate": 4.991768605001507e-06,
      "loss": 1.9369,
      "step": 2050
    },
    {
      "epoch": 0.004965351009340163,
      "grad_norm": 21.168397903442383,
      "learning_rate": 4.9917284322587125e-06,
      "loss": 2.527,
      "step": 2060
    },
    {
      "epoch": 0.004989454655016571,
      "grad_norm": 18.051584243774414,
      "learning_rate": 4.991688259515919e-06,
      "loss": 1.7668,
      "step": 2070
    },
    {
      "epoch": 0.00501355830069298,
      "grad_norm": 17.731700897216797,
      "learning_rate": 4.991648086773125e-06,
      "loss": 2.0087,
      "step": 2080
    },
    {
      "epoch": 0.005037661946369388,
      "grad_norm": 21.143117904663086,
      "learning_rate": 4.991607914030331e-06,
      "loss": 2.2781,
      "step": 2090
    },
    {
      "epoch": 0.005061765592045797,
      "grad_norm": 16.34496307373047,
      "learning_rate": 4.991567741287537e-06,
      "loss": 2.8312,
      "step": 2100
    },
    {
      "epoch": 0.0050858692377222054,
      "grad_norm": 40.20743942260742,
      "learning_rate": 4.991527568544743e-06,
      "loss": 2.8053,
      "step": 2110
    },
    {
      "epoch": 0.005109972883398614,
      "grad_norm": 13.224532127380371,
      "learning_rate": 4.9914873958019485e-06,
      "loss": 2.4413,
      "step": 2120
    },
    {
      "epoch": 0.005134076529075023,
      "grad_norm": 18.159709930419922,
      "learning_rate": 4.991447223059155e-06,
      "loss": 2.1042,
      "step": 2130
    },
    {
      "epoch": 0.005158180174751431,
      "grad_norm": 23.867061614990234,
      "learning_rate": 4.991407050316361e-06,
      "loss": 2.0879,
      "step": 2140
    },
    {
      "epoch": 0.00518228382042784,
      "grad_norm": 27.55080795288086,
      "learning_rate": 4.991366877573567e-06,
      "loss": 2.434,
      "step": 2150
    },
    {
      "epoch": 0.005206387466104248,
      "grad_norm": 17.043006896972656,
      "learning_rate": 4.991326704830773e-06,
      "loss": 3.0446,
      "step": 2160
    },
    {
      "epoch": 0.005230491111780657,
      "grad_norm": 14.45619010925293,
      "learning_rate": 4.991286532087979e-06,
      "loss": 1.9337,
      "step": 2170
    },
    {
      "epoch": 0.005254594757457065,
      "grad_norm": 35.37712478637695,
      "learning_rate": 4.9912463593451844e-06,
      "loss": 2.5653,
      "step": 2180
    },
    {
      "epoch": 0.005278698403133474,
      "grad_norm": 18.25859260559082,
      "learning_rate": 4.991206186602391e-06,
      "loss": 2.2563,
      "step": 2190
    },
    {
      "epoch": 0.005302802048809883,
      "grad_norm": 21.31658935546875,
      "learning_rate": 4.991166013859597e-06,
      "loss": 2.5759,
      "step": 2200
    },
    {
      "epoch": 0.005326905694486291,
      "grad_norm": 24.17500877380371,
      "learning_rate": 4.991125841116803e-06,
      "loss": 2.8292,
      "step": 2210
    },
    {
      "epoch": 0.0053510093401626995,
      "grad_norm": 26.008014678955078,
      "learning_rate": 4.991085668374009e-06,
      "loss": 2.0123,
      "step": 2220
    },
    {
      "epoch": 0.005375112985839108,
      "grad_norm": 20.805078506469727,
      "learning_rate": 4.991045495631215e-06,
      "loss": 2.382,
      "step": 2230
    },
    {
      "epoch": 0.005399216631515517,
      "grad_norm": 17.637638092041016,
      "learning_rate": 4.991005322888421e-06,
      "loss": 2.7765,
      "step": 2240
    },
    {
      "epoch": 0.005423320277191925,
      "grad_norm": 16.80082130432129,
      "learning_rate": 4.990965150145626e-06,
      "loss": 2.3918,
      "step": 2250
    },
    {
      "epoch": 0.005447423922868334,
      "grad_norm": 24.573976516723633,
      "learning_rate": 4.990924977402832e-06,
      "loss": 2.2046,
      "step": 2260
    },
    {
      "epoch": 0.005471527568544742,
      "grad_norm": 20.44346809387207,
      "learning_rate": 4.990884804660038e-06,
      "loss": 2.5141,
      "step": 2270
    },
    {
      "epoch": 0.005495631214221151,
      "grad_norm": 30.78228759765625,
      "learning_rate": 4.990844631917245e-06,
      "loss": 2.7279,
      "step": 2280
    },
    {
      "epoch": 0.0055197348598975595,
      "grad_norm": 21.884735107421875,
      "learning_rate": 4.9908044591744506e-06,
      "loss": 2.0962,
      "step": 2290
    },
    {
      "epoch": 0.005543838505573968,
      "grad_norm": 9.907788276672363,
      "learning_rate": 4.9907642864316564e-06,
      "loss": 2.3812,
      "step": 2300
    },
    {
      "epoch": 0.005567942151250377,
      "grad_norm": 23.98328971862793,
      "learning_rate": 4.990724113688862e-06,
      "loss": 2.2163,
      "step": 2310
    },
    {
      "epoch": 0.005592045796926785,
      "grad_norm": 19.949996948242188,
      "learning_rate": 4.990683940946068e-06,
      "loss": 2.4938,
      "step": 2320
    },
    {
      "epoch": 0.005616149442603194,
      "grad_norm": 15.867815971374512,
      "learning_rate": 4.990643768203274e-06,
      "loss": 2.4378,
      "step": 2330
    },
    {
      "epoch": 0.005640253088279602,
      "grad_norm": 23.833106994628906,
      "learning_rate": 4.990603595460481e-06,
      "loss": 2.1857,
      "step": 2340
    },
    {
      "epoch": 0.005664356733956011,
      "grad_norm": 11.726542472839355,
      "learning_rate": 4.9905634227176866e-06,
      "loss": 2.4584,
      "step": 2350
    },
    {
      "epoch": 0.0056884603796324195,
      "grad_norm": 31.0660400390625,
      "learning_rate": 4.9905232499748924e-06,
      "loss": 2.3588,
      "step": 2360
    },
    {
      "epoch": 0.005712564025308828,
      "grad_norm": 14.42349910736084,
      "learning_rate": 4.990483077232098e-06,
      "loss": 2.2154,
      "step": 2370
    },
    {
      "epoch": 0.005736667670985236,
      "grad_norm": 24.69171905517578,
      "learning_rate": 4.990442904489304e-06,
      "loss": 2.322,
      "step": 2380
    },
    {
      "epoch": 0.005760771316661645,
      "grad_norm": 21.74642562866211,
      "learning_rate": 4.99040273174651e-06,
      "loss": 2.0048,
      "step": 2390
    },
    {
      "epoch": 0.005784874962338054,
      "grad_norm": 15.455157279968262,
      "learning_rate": 4.990362559003717e-06,
      "loss": 2.0664,
      "step": 2400
    },
    {
      "epoch": 0.005808978608014462,
      "grad_norm": 22.340681076049805,
      "learning_rate": 4.9903223862609226e-06,
      "loss": 2.2945,
      "step": 2410
    },
    {
      "epoch": 0.005833082253690871,
      "grad_norm": 23.773162841796875,
      "learning_rate": 4.990282213518128e-06,
      "loss": 2.241,
      "step": 2420
    },
    {
      "epoch": 0.005857185899367279,
      "grad_norm": 30.537477493286133,
      "learning_rate": 4.990242040775334e-06,
      "loss": 2.4924,
      "step": 2430
    },
    {
      "epoch": 0.005881289545043688,
      "grad_norm": 20.74516487121582,
      "learning_rate": 4.99020186803254e-06,
      "loss": 1.9764,
      "step": 2440
    },
    {
      "epoch": 0.005905393190720096,
      "grad_norm": 11.694893836975098,
      "learning_rate": 4.990161695289746e-06,
      "loss": 1.8608,
      "step": 2450
    },
    {
      "epoch": 0.005929496836396505,
      "grad_norm": 18.66691017150879,
      "learning_rate": 4.990121522546953e-06,
      "loss": 2.4577,
      "step": 2460
    },
    {
      "epoch": 0.005953600482072914,
      "grad_norm": 21.88396644592285,
      "learning_rate": 4.9900813498041586e-06,
      "loss": 1.9924,
      "step": 2470
    },
    {
      "epoch": 0.005977704127749322,
      "grad_norm": 19.842042922973633,
      "learning_rate": 4.990041177061364e-06,
      "loss": 2.5228,
      "step": 2480
    },
    {
      "epoch": 0.006001807773425731,
      "grad_norm": 11.972067832946777,
      "learning_rate": 4.99000100431857e-06,
      "loss": 2.3324,
      "step": 2490
    },
    {
      "epoch": 0.006025911419102139,
      "grad_norm": 27.531211853027344,
      "learning_rate": 4.989960831575776e-06,
      "loss": 2.4436,
      "step": 2500
    },
    {
      "epoch": 0.006050015064778548,
      "grad_norm": 16.729211807250977,
      "learning_rate": 4.989920658832982e-06,
      "loss": 1.7931,
      "step": 2510
    },
    {
      "epoch": 0.006074118710454956,
      "grad_norm": 13.872501373291016,
      "learning_rate": 4.989880486090189e-06,
      "loss": 1.5152,
      "step": 2520
    },
    {
      "epoch": 0.006098222356131365,
      "grad_norm": 17.242582321166992,
      "learning_rate": 4.9898403133473946e-06,
      "loss": 1.8696,
      "step": 2530
    },
    {
      "epoch": 0.0061223260018077735,
      "grad_norm": 47.10820770263672,
      "learning_rate": 4.9898001406046e-06,
      "loss": 2.5464,
      "step": 2540
    },
    {
      "epoch": 0.006146429647484182,
      "grad_norm": 23.142980575561523,
      "learning_rate": 4.989759967861806e-06,
      "loss": 1.7316,
      "step": 2550
    },
    {
      "epoch": 0.00617053329316059,
      "grad_norm": 20.580793380737305,
      "learning_rate": 4.989719795119012e-06,
      "loss": 2.5221,
      "step": 2560
    },
    {
      "epoch": 0.006194636938836999,
      "grad_norm": 14.349837303161621,
      "learning_rate": 4.989679622376218e-06,
      "loss": 2.0308,
      "step": 2570
    },
    {
      "epoch": 0.006218740584513408,
      "grad_norm": 27.24531364440918,
      "learning_rate": 4.989639449633425e-06,
      "loss": 3.3949,
      "step": 2580
    },
    {
      "epoch": 0.006242844230189816,
      "grad_norm": 20.994054794311523,
      "learning_rate": 4.98959927689063e-06,
      "loss": 1.8902,
      "step": 2590
    },
    {
      "epoch": 0.006266947875866225,
      "grad_norm": 33.63188934326172,
      "learning_rate": 4.9895591041478356e-06,
      "loss": 2.2275,
      "step": 2600
    },
    {
      "epoch": 0.0062910515215426335,
      "grad_norm": 20.540525436401367,
      "learning_rate": 4.989518931405042e-06,
      "loss": 2.1214,
      "step": 2610
    },
    {
      "epoch": 0.006315155167219042,
      "grad_norm": 15.68220329284668,
      "learning_rate": 4.989478758662248e-06,
      "loss": 2.5687,
      "step": 2620
    },
    {
      "epoch": 0.00633925881289545,
      "grad_norm": 21.46030616760254,
      "learning_rate": 4.989438585919454e-06,
      "loss": 2.2809,
      "step": 2630
    },
    {
      "epoch": 0.006363362458571859,
      "grad_norm": 26.132633209228516,
      "learning_rate": 4.98939841317666e-06,
      "loss": 2.074,
      "step": 2640
    },
    {
      "epoch": 0.006387466104248268,
      "grad_norm": 23.25284194946289,
      "learning_rate": 4.989358240433866e-06,
      "loss": 2.3719,
      "step": 2650
    },
    {
      "epoch": 0.006411569749924676,
      "grad_norm": 20.2139835357666,
      "learning_rate": 4.9893180676910716e-06,
      "loss": 2.4849,
      "step": 2660
    },
    {
      "epoch": 0.006435673395601084,
      "grad_norm": 10.637961387634277,
      "learning_rate": 4.989277894948278e-06,
      "loss": 1.8552,
      "step": 2670
    },
    {
      "epoch": 0.0064597770412774935,
      "grad_norm": 24.795085906982422,
      "learning_rate": 4.989237722205484e-06,
      "loss": 2.1324,
      "step": 2680
    },
    {
      "epoch": 0.006483880686953902,
      "grad_norm": 19.544599533081055,
      "learning_rate": 4.98919754946269e-06,
      "loss": 2.1784,
      "step": 2690
    },
    {
      "epoch": 0.00650798433263031,
      "grad_norm": 21.506982803344727,
      "learning_rate": 4.989157376719896e-06,
      "loss": 1.7239,
      "step": 2700
    },
    {
      "epoch": 0.006532087978306719,
      "grad_norm": 41.829925537109375,
      "learning_rate": 4.989117203977102e-06,
      "loss": 2.0746,
      "step": 2710
    },
    {
      "epoch": 0.006556191623983128,
      "grad_norm": 26.116756439208984,
      "learning_rate": 4.9890770312343075e-06,
      "loss": 2.6019,
      "step": 2720
    },
    {
      "epoch": 0.006580295269659536,
      "grad_norm": 22.822092056274414,
      "learning_rate": 4.989036858491514e-06,
      "loss": 1.8667,
      "step": 2730
    },
    {
      "epoch": 0.006604398915335944,
      "grad_norm": 24.894819259643555,
      "learning_rate": 4.98899668574872e-06,
      "loss": 2.0791,
      "step": 2740
    },
    {
      "epoch": 0.006628502561012353,
      "grad_norm": 20.05602264404297,
      "learning_rate": 4.988956513005926e-06,
      "loss": 2.3556,
      "step": 2750
    },
    {
      "epoch": 0.006652606206688762,
      "grad_norm": 14.851665496826172,
      "learning_rate": 4.988916340263132e-06,
      "loss": 2.3663,
      "step": 2760
    },
    {
      "epoch": 0.00667670985236517,
      "grad_norm": 12.941082954406738,
      "learning_rate": 4.988876167520338e-06,
      "loss": 2.4449,
      "step": 2770
    },
    {
      "epoch": 0.006700813498041578,
      "grad_norm": 9.156922340393066,
      "learning_rate": 4.9888359947775435e-06,
      "loss": 2.2277,
      "step": 2780
    },
    {
      "epoch": 0.0067249171437179876,
      "grad_norm": 13.857565879821777,
      "learning_rate": 4.98879582203475e-06,
      "loss": 2.0101,
      "step": 2790
    },
    {
      "epoch": 0.006749020789394396,
      "grad_norm": 18.567811965942383,
      "learning_rate": 4.988755649291956e-06,
      "loss": 2.3108,
      "step": 2800
    },
    {
      "epoch": 0.006773124435070804,
      "grad_norm": 13.557140350341797,
      "learning_rate": 4.988715476549162e-06,
      "loss": 2.1215,
      "step": 2810
    },
    {
      "epoch": 0.006797228080747213,
      "grad_norm": 24.945106506347656,
      "learning_rate": 4.988675303806368e-06,
      "loss": 2.3799,
      "step": 2820
    },
    {
      "epoch": 0.006821331726423622,
      "grad_norm": 18.458980560302734,
      "learning_rate": 4.988635131063574e-06,
      "loss": 1.9115,
      "step": 2830
    },
    {
      "epoch": 0.00684543537210003,
      "grad_norm": 19.037830352783203,
      "learning_rate": 4.9885949583207795e-06,
      "loss": 2.29,
      "step": 2840
    },
    {
      "epoch": 0.006869539017776438,
      "grad_norm": 12.454587936401367,
      "learning_rate": 4.988554785577986e-06,
      "loss": 1.898,
      "step": 2850
    },
    {
      "epoch": 0.0068936426634528475,
      "grad_norm": 17.257122039794922,
      "learning_rate": 4.988514612835192e-06,
      "loss": 2.1229,
      "step": 2860
    },
    {
      "epoch": 0.006917746309129256,
      "grad_norm": 11.018630027770996,
      "learning_rate": 4.988474440092398e-06,
      "loss": 1.8091,
      "step": 2870
    },
    {
      "epoch": 0.006941849954805664,
      "grad_norm": 18.817962646484375,
      "learning_rate": 4.988434267349604e-06,
      "loss": 2.2203,
      "step": 2880
    },
    {
      "epoch": 0.006965953600482073,
      "grad_norm": 25.641265869140625,
      "learning_rate": 4.98839409460681e-06,
      "loss": 2.3686,
      "step": 2890
    },
    {
      "epoch": 0.006990057246158482,
      "grad_norm": 15.040046691894531,
      "learning_rate": 4.9883539218640155e-06,
      "loss": 2.4219,
      "step": 2900
    },
    {
      "epoch": 0.00701416089183489,
      "grad_norm": 18.142322540283203,
      "learning_rate": 4.988313749121222e-06,
      "loss": 1.7311,
      "step": 2910
    },
    {
      "epoch": 0.007038264537511298,
      "grad_norm": 22.795820236206055,
      "learning_rate": 4.988273576378428e-06,
      "loss": 2.2123,
      "step": 2920
    },
    {
      "epoch": 0.0070623681831877075,
      "grad_norm": 16.714750289916992,
      "learning_rate": 4.988233403635633e-06,
      "loss": 1.6959,
      "step": 2930
    },
    {
      "epoch": 0.007086471828864116,
      "grad_norm": 12.302002906799316,
      "learning_rate": 4.988193230892839e-06,
      "loss": 1.9751,
      "step": 2940
    },
    {
      "epoch": 0.007110575474540524,
      "grad_norm": 18.39186668395996,
      "learning_rate": 4.988153058150046e-06,
      "loss": 2.815,
      "step": 2950
    },
    {
      "epoch": 0.0071346791202169324,
      "grad_norm": 22.438770294189453,
      "learning_rate": 4.9881128854072515e-06,
      "loss": 2.6957,
      "step": 2960
    },
    {
      "epoch": 0.007158782765893342,
      "grad_norm": 14.859845161437988,
      "learning_rate": 4.988072712664457e-06,
      "loss": 2.2912,
      "step": 2970
    },
    {
      "epoch": 0.00718288641156975,
      "grad_norm": 17.01802635192871,
      "learning_rate": 4.988032539921663e-06,
      "loss": 2.5328,
      "step": 2980
    },
    {
      "epoch": 0.007206990057246158,
      "grad_norm": 17.963825225830078,
      "learning_rate": 4.987992367178869e-06,
      "loss": 2.3621,
      "step": 2990
    },
    {
      "epoch": 0.0072310937029225674,
      "grad_norm": 24.452058792114258,
      "learning_rate": 4.987952194436075e-06,
      "loss": 2.9349,
      "step": 3000
    },
    {
      "epoch": 0.007255197348598976,
      "grad_norm": 17.334075927734375,
      "learning_rate": 4.987912021693282e-06,
      "loss": 2.5537,
      "step": 3010
    },
    {
      "epoch": 0.007279300994275384,
      "grad_norm": 23.775531768798828,
      "learning_rate": 4.9878718489504875e-06,
      "loss": 1.8874,
      "step": 3020
    },
    {
      "epoch": 0.007303404639951792,
      "grad_norm": 25.216106414794922,
      "learning_rate": 4.987831676207693e-06,
      "loss": 2.1412,
      "step": 3030
    },
    {
      "epoch": 0.007327508285628202,
      "grad_norm": 7.032816410064697,
      "learning_rate": 4.987791503464899e-06,
      "loss": 2.2528,
      "step": 3040
    },
    {
      "epoch": 0.00735161193130461,
      "grad_norm": 25.503677368164062,
      "learning_rate": 4.987751330722105e-06,
      "loss": 1.6519,
      "step": 3050
    },
    {
      "epoch": 0.007375715576981018,
      "grad_norm": 23.81955337524414,
      "learning_rate": 4.987711157979312e-06,
      "loss": 1.829,
      "step": 3060
    },
    {
      "epoch": 0.0073998192226574265,
      "grad_norm": 13.68187141418457,
      "learning_rate": 4.987670985236518e-06,
      "loss": 1.6685,
      "step": 3070
    },
    {
      "epoch": 0.007423922868333836,
      "grad_norm": 13.658677101135254,
      "learning_rate": 4.9876308124937235e-06,
      "loss": 1.6309,
      "step": 3080
    },
    {
      "epoch": 0.007448026514010244,
      "grad_norm": 22.103307723999023,
      "learning_rate": 4.987590639750929e-06,
      "loss": 2.1857,
      "step": 3090
    },
    {
      "epoch": 0.007472130159686652,
      "grad_norm": 21.645153045654297,
      "learning_rate": 4.987550467008135e-06,
      "loss": 1.9466,
      "step": 3100
    },
    {
      "epoch": 0.0074962338053630615,
      "grad_norm": 12.310310363769531,
      "learning_rate": 4.987510294265341e-06,
      "loss": 2.1747,
      "step": 3110
    },
    {
      "epoch": 0.00752033745103947,
      "grad_norm": 19.26943016052246,
      "learning_rate": 4.987470121522548e-06,
      "loss": 1.7238,
      "step": 3120
    },
    {
      "epoch": 0.007544441096715878,
      "grad_norm": 20.437408447265625,
      "learning_rate": 4.987429948779754e-06,
      "loss": 2.1784,
      "step": 3130
    },
    {
      "epoch": 0.0075685447423922865,
      "grad_norm": 29.83953857421875,
      "learning_rate": 4.9873897760369595e-06,
      "loss": 1.9473,
      "step": 3140
    },
    {
      "epoch": 0.007592648388068696,
      "grad_norm": 25.547584533691406,
      "learning_rate": 4.987349603294165e-06,
      "loss": 2.1467,
      "step": 3150
    },
    {
      "epoch": 0.007616752033745104,
      "grad_norm": 30.700096130371094,
      "learning_rate": 4.987309430551371e-06,
      "loss": 2.4262,
      "step": 3160
    },
    {
      "epoch": 0.007640855679421512,
      "grad_norm": 18.811433792114258,
      "learning_rate": 4.987269257808577e-06,
      "loss": 2.3268,
      "step": 3170
    },
    {
      "epoch": 0.007664959325097921,
      "grad_norm": 19.678470611572266,
      "learning_rate": 4.987229085065784e-06,
      "loss": 2.3833,
      "step": 3180
    },
    {
      "epoch": 0.00768906297077433,
      "grad_norm": 37.42119598388672,
      "learning_rate": 4.98718891232299e-06,
      "loss": 1.8813,
      "step": 3190
    },
    {
      "epoch": 0.007713166616450738,
      "grad_norm": 11.951313018798828,
      "learning_rate": 4.9871487395801955e-06,
      "loss": 2.7211,
      "step": 3200
    },
    {
      "epoch": 0.0077372702621271465,
      "grad_norm": 19.40114974975586,
      "learning_rate": 4.987108566837401e-06,
      "loss": 2.5381,
      "step": 3210
    },
    {
      "epoch": 0.007761373907803556,
      "grad_norm": 22.3049373626709,
      "learning_rate": 4.987068394094607e-06,
      "loss": 1.6965,
      "step": 3220
    },
    {
      "epoch": 0.007785477553479964,
      "grad_norm": 20.255020141601562,
      "learning_rate": 4.987028221351813e-06,
      "loss": 1.9894,
      "step": 3230
    },
    {
      "epoch": 0.007809581199156372,
      "grad_norm": 23.096078872680664,
      "learning_rate": 4.98698804860902e-06,
      "loss": 1.8503,
      "step": 3240
    },
    {
      "epoch": 0.00783368484483278,
      "grad_norm": 15.24679946899414,
      "learning_rate": 4.986947875866226e-06,
      "loss": 2.2334,
      "step": 3250
    },
    {
      "epoch": 0.00785778849050919,
      "grad_norm": 18.9800968170166,
      "learning_rate": 4.9869077031234315e-06,
      "loss": 2.614,
      "step": 3260
    },
    {
      "epoch": 0.007881892136185597,
      "grad_norm": 3.2227225303649902,
      "learning_rate": 4.986867530380637e-06,
      "loss": 1.8048,
      "step": 3270
    },
    {
      "epoch": 0.007905995781862006,
      "grad_norm": 15.325238227844238,
      "learning_rate": 4.986827357637843e-06,
      "loss": 1.8655,
      "step": 3280
    },
    {
      "epoch": 0.007930099427538416,
      "grad_norm": 20.45035743713379,
      "learning_rate": 4.986787184895049e-06,
      "loss": 1.9912,
      "step": 3290
    },
    {
      "epoch": 0.007954203073214823,
      "grad_norm": 29.92798614501953,
      "learning_rate": 4.986747012152255e-06,
      "loss": 2.1679,
      "step": 3300
    },
    {
      "epoch": 0.007978306718891232,
      "grad_norm": 16.35470962524414,
      "learning_rate": 4.986706839409461e-06,
      "loss": 2.2349,
      "step": 3310
    },
    {
      "epoch": 0.008002410364567641,
      "grad_norm": 7.681275844573975,
      "learning_rate": 4.986666666666667e-06,
      "loss": 2.1617,
      "step": 3320
    },
    {
      "epoch": 0.008026514010244049,
      "grad_norm": 28.974489212036133,
      "learning_rate": 4.9866264939238725e-06,
      "loss": 2.216,
      "step": 3330
    },
    {
      "epoch": 0.008050617655920458,
      "grad_norm": 19.582481384277344,
      "learning_rate": 4.986586321181079e-06,
      "loss": 1.9769,
      "step": 3340
    },
    {
      "epoch": 0.008074721301596867,
      "grad_norm": 18.092451095581055,
      "learning_rate": 4.986546148438285e-06,
      "loss": 2.2485,
      "step": 3350
    },
    {
      "epoch": 0.008098824947273275,
      "grad_norm": 6.711248874664307,
      "learning_rate": 4.986505975695491e-06,
      "loss": 2.2425,
      "step": 3360
    },
    {
      "epoch": 0.008122928592949684,
      "grad_norm": 18.885696411132812,
      "learning_rate": 4.986465802952697e-06,
      "loss": 2.0859,
      "step": 3370
    },
    {
      "epoch": 0.008147032238626091,
      "grad_norm": 29.44808578491211,
      "learning_rate": 4.986425630209903e-06,
      "loss": 1.7832,
      "step": 3380
    },
    {
      "epoch": 0.0081711358843025,
      "grad_norm": 16.197364807128906,
      "learning_rate": 4.9863854574671085e-06,
      "loss": 2.4483,
      "step": 3390
    },
    {
      "epoch": 0.00819523952997891,
      "grad_norm": 27.6533260345459,
      "learning_rate": 4.986345284724315e-06,
      "loss": 2.1163,
      "step": 3400
    },
    {
      "epoch": 0.008219343175655317,
      "grad_norm": 30.639450073242188,
      "learning_rate": 4.986305111981521e-06,
      "loss": 1.7483,
      "step": 3410
    },
    {
      "epoch": 0.008243446821331726,
      "grad_norm": 15.762425422668457,
      "learning_rate": 4.986264939238727e-06,
      "loss": 2.1853,
      "step": 3420
    },
    {
      "epoch": 0.008267550467008136,
      "grad_norm": 24.107269287109375,
      "learning_rate": 4.986224766495933e-06,
      "loss": 1.9471,
      "step": 3430
    },
    {
      "epoch": 0.008291654112684543,
      "grad_norm": 22.093191146850586,
      "learning_rate": 4.986184593753139e-06,
      "loss": 2.3502,
      "step": 3440
    },
    {
      "epoch": 0.008315757758360952,
      "grad_norm": 28.088890075683594,
      "learning_rate": 4.9861444210103445e-06,
      "loss": 2.3506,
      "step": 3450
    },
    {
      "epoch": 0.008339861404037361,
      "grad_norm": 25.248634338378906,
      "learning_rate": 4.986104248267551e-06,
      "loss": 2.176,
      "step": 3460
    },
    {
      "epoch": 0.008363965049713769,
      "grad_norm": 19.155912399291992,
      "learning_rate": 4.986064075524757e-06,
      "loss": 2.1504,
      "step": 3470
    },
    {
      "epoch": 0.008388068695390178,
      "grad_norm": 39.368587493896484,
      "learning_rate": 4.986023902781963e-06,
      "loss": 2.0739,
      "step": 3480
    },
    {
      "epoch": 0.008412172341066587,
      "grad_norm": 21.332319259643555,
      "learning_rate": 4.985983730039169e-06,
      "loss": 2.6314,
      "step": 3490
    },
    {
      "epoch": 0.008436275986742995,
      "grad_norm": 21.812822341918945,
      "learning_rate": 4.985943557296375e-06,
      "loss": 2.3959,
      "step": 3500
    },
    {
      "epoch": 0.008460379632419404,
      "grad_norm": 16.98487091064453,
      "learning_rate": 4.985903384553581e-06,
      "loss": 2.2344,
      "step": 3510
    },
    {
      "epoch": 0.008484483278095811,
      "grad_norm": 23.557594299316406,
      "learning_rate": 4.985863211810787e-06,
      "loss": 2.0617,
      "step": 3520
    },
    {
      "epoch": 0.00850858692377222,
      "grad_norm": 12.088746070861816,
      "learning_rate": 4.985823039067993e-06,
      "loss": 1.7539,
      "step": 3530
    },
    {
      "epoch": 0.00853269056944863,
      "grad_norm": 13.880462646484375,
      "learning_rate": 4.985782866325199e-06,
      "loss": 1.9209,
      "step": 3540
    },
    {
      "epoch": 0.008556794215125037,
      "grad_norm": 15.415026664733887,
      "learning_rate": 4.985742693582405e-06,
      "loss": 2.556,
      "step": 3550
    },
    {
      "epoch": 0.008580897860801446,
      "grad_norm": 17.24401092529297,
      "learning_rate": 4.985702520839611e-06,
      "loss": 1.9674,
      "step": 3560
    },
    {
      "epoch": 0.008605001506477855,
      "grad_norm": 29.278989791870117,
      "learning_rate": 4.985662348096817e-06,
      "loss": 2.1942,
      "step": 3570
    },
    {
      "epoch": 0.008629105152154263,
      "grad_norm": 11.238872528076172,
      "learning_rate": 4.985622175354023e-06,
      "loss": 2.3917,
      "step": 3580
    },
    {
      "epoch": 0.008653208797830672,
      "grad_norm": 19.113588333129883,
      "learning_rate": 4.985582002611229e-06,
      "loss": 1.8987,
      "step": 3590
    },
    {
      "epoch": 0.008677312443507081,
      "grad_norm": 6.377508640289307,
      "learning_rate": 4.985541829868435e-06,
      "loss": 2.3029,
      "step": 3600
    },
    {
      "epoch": 0.008701416089183489,
      "grad_norm": 19.976308822631836,
      "learning_rate": 4.985501657125641e-06,
      "loss": 1.8085,
      "step": 3610
    },
    {
      "epoch": 0.008725519734859898,
      "grad_norm": 16.576671600341797,
      "learning_rate": 4.985461484382847e-06,
      "loss": 1.9675,
      "step": 3620
    },
    {
      "epoch": 0.008749623380536305,
      "grad_norm": 20.551301956176758,
      "learning_rate": 4.9854213116400525e-06,
      "loss": 2.6116,
      "step": 3630
    },
    {
      "epoch": 0.008773727026212715,
      "grad_norm": 18.02307891845703,
      "learning_rate": 4.985381138897258e-06,
      "loss": 2.0072,
      "step": 3640
    },
    {
      "epoch": 0.008797830671889124,
      "grad_norm": 21.735610961914062,
      "learning_rate": 4.985340966154464e-06,
      "loss": 2.5964,
      "step": 3650
    },
    {
      "epoch": 0.008821934317565531,
      "grad_norm": 13.214042663574219,
      "learning_rate": 4.98530079341167e-06,
      "loss": 1.6616,
      "step": 3660
    },
    {
      "epoch": 0.00884603796324194,
      "grad_norm": 15.84788990020752,
      "learning_rate": 4.985260620668877e-06,
      "loss": 1.8405,
      "step": 3670
    },
    {
      "epoch": 0.00887014160891835,
      "grad_norm": 10.504998207092285,
      "learning_rate": 4.985220447926083e-06,
      "loss": 2.2187,
      "step": 3680
    },
    {
      "epoch": 0.008894245254594757,
      "grad_norm": 18.414522171020508,
      "learning_rate": 4.9851802751832885e-06,
      "loss": 1.6986,
      "step": 3690
    },
    {
      "epoch": 0.008918348900271166,
      "grad_norm": 15.35141658782959,
      "learning_rate": 4.985140102440494e-06,
      "loss": 1.7517,
      "step": 3700
    },
    {
      "epoch": 0.008942452545947575,
      "grad_norm": 34.60067367553711,
      "learning_rate": 4.9850999296977e-06,
      "loss": 2.3491,
      "step": 3710
    },
    {
      "epoch": 0.008966556191623983,
      "grad_norm": 5.230729579925537,
      "learning_rate": 4.985059756954906e-06,
      "loss": 1.9813,
      "step": 3720
    },
    {
      "epoch": 0.008990659837300392,
      "grad_norm": 12.138106346130371,
      "learning_rate": 4.985019584212113e-06,
      "loss": 2.1005,
      "step": 3730
    },
    {
      "epoch": 0.0090147634829768,
      "grad_norm": 17.969274520874023,
      "learning_rate": 4.984979411469319e-06,
      "loss": 2.2599,
      "step": 3740
    },
    {
      "epoch": 0.009038867128653209,
      "grad_norm": 19.941129684448242,
      "learning_rate": 4.9849392387265245e-06,
      "loss": 2.3309,
      "step": 3750
    },
    {
      "epoch": 0.009062970774329618,
      "grad_norm": 35.63772201538086,
      "learning_rate": 4.98489906598373e-06,
      "loss": 2.2458,
      "step": 3760
    },
    {
      "epoch": 0.009087074420006025,
      "grad_norm": 17.546112060546875,
      "learning_rate": 4.984858893240936e-06,
      "loss": 2.4258,
      "step": 3770
    },
    {
      "epoch": 0.009111178065682434,
      "grad_norm": 20.207460403442383,
      "learning_rate": 4.984818720498142e-06,
      "loss": 2.4028,
      "step": 3780
    },
    {
      "epoch": 0.009135281711358844,
      "grad_norm": 18.80498695373535,
      "learning_rate": 4.984778547755349e-06,
      "loss": 2.003,
      "step": 3790
    },
    {
      "epoch": 0.009159385357035251,
      "grad_norm": 32.29022216796875,
      "learning_rate": 4.984738375012555e-06,
      "loss": 1.8411,
      "step": 3800
    },
    {
      "epoch": 0.00918348900271166,
      "grad_norm": 47.30104064941406,
      "learning_rate": 4.9846982022697604e-06,
      "loss": 2.6233,
      "step": 3810
    },
    {
      "epoch": 0.00920759264838807,
      "grad_norm": 17.305992126464844,
      "learning_rate": 4.984658029526966e-06,
      "loss": 1.8103,
      "step": 3820
    },
    {
      "epoch": 0.009231696294064477,
      "grad_norm": 13.669087409973145,
      "learning_rate": 4.984617856784172e-06,
      "loss": 2.1423,
      "step": 3830
    },
    {
      "epoch": 0.009255799939740886,
      "grad_norm": 18.48644256591797,
      "learning_rate": 4.984577684041378e-06,
      "loss": 2.4102,
      "step": 3840
    },
    {
      "epoch": 0.009279903585417294,
      "grad_norm": 18.866958618164062,
      "learning_rate": 4.984537511298585e-06,
      "loss": 2.1594,
      "step": 3850
    },
    {
      "epoch": 0.009304007231093703,
      "grad_norm": 20.36957550048828,
      "learning_rate": 4.984497338555791e-06,
      "loss": 2.454,
      "step": 3860
    },
    {
      "epoch": 0.009328110876770112,
      "grad_norm": 19.516294479370117,
      "learning_rate": 4.9844571658129964e-06,
      "loss": 1.941,
      "step": 3870
    },
    {
      "epoch": 0.00935221452244652,
      "grad_norm": 18.814424514770508,
      "learning_rate": 4.984416993070202e-06,
      "loss": 1.8842,
      "step": 3880
    },
    {
      "epoch": 0.009376318168122929,
      "grad_norm": 11.279279708862305,
      "learning_rate": 4.984376820327408e-06,
      "loss": 1.4828,
      "step": 3890
    },
    {
      "epoch": 0.009400421813799338,
      "grad_norm": 14.463990211486816,
      "learning_rate": 4.984336647584615e-06,
      "loss": 2.1752,
      "step": 3900
    },
    {
      "epoch": 0.009424525459475745,
      "grad_norm": 15.063093185424805,
      "learning_rate": 4.984296474841821e-06,
      "loss": 1.6747,
      "step": 3910
    },
    {
      "epoch": 0.009448629105152154,
      "grad_norm": 22.470550537109375,
      "learning_rate": 4.984256302099027e-06,
      "loss": 2.1482,
      "step": 3920
    },
    {
      "epoch": 0.009472732750828564,
      "grad_norm": 4.907624244689941,
      "learning_rate": 4.9842161293562324e-06,
      "loss": 1.7965,
      "step": 3930
    },
    {
      "epoch": 0.009496836396504971,
      "grad_norm": 29.5640811920166,
      "learning_rate": 4.984175956613438e-06,
      "loss": 2.046,
      "step": 3940
    },
    {
      "epoch": 0.00952094004218138,
      "grad_norm": 24.573902130126953,
      "learning_rate": 4.984135783870644e-06,
      "loss": 2.1926,
      "step": 3950
    },
    {
      "epoch": 0.009545043687857788,
      "grad_norm": 16.044374465942383,
      "learning_rate": 4.98409561112785e-06,
      "loss": 1.726,
      "step": 3960
    },
    {
      "epoch": 0.009569147333534197,
      "grad_norm": 12.493169784545898,
      "learning_rate": 4.984055438385056e-06,
      "loss": 1.8441,
      "step": 3970
    },
    {
      "epoch": 0.009593250979210606,
      "grad_norm": 26.502193450927734,
      "learning_rate": 4.984015265642262e-06,
      "loss": 1.8536,
      "step": 3980
    },
    {
      "epoch": 0.009617354624887013,
      "grad_norm": 55.21099853515625,
      "learning_rate": 4.983975092899468e-06,
      "loss": 2.2572,
      "step": 3990
    },
    {
      "epoch": 0.009641458270563423,
      "grad_norm": 20.246116638183594,
      "learning_rate": 4.983934920156674e-06,
      "loss": 2.1173,
      "step": 4000
    },
    {
      "epoch": 0.009665561916239832,
      "grad_norm": 12.398451805114746,
      "learning_rate": 4.98389474741388e-06,
      "loss": 1.8325,
      "step": 4010
    },
    {
      "epoch": 0.00968966556191624,
      "grad_norm": 27.476938247680664,
      "learning_rate": 4.983854574671086e-06,
      "loss": 1.9475,
      "step": 4020
    },
    {
      "epoch": 0.009713769207592649,
      "grad_norm": 19.103858947753906,
      "learning_rate": 4.983814401928292e-06,
      "loss": 1.9148,
      "step": 4030
    },
    {
      "epoch": 0.009737872853269058,
      "grad_norm": 38.06053924560547,
      "learning_rate": 4.983774229185498e-06,
      "loss": 2.2295,
      "step": 4040
    },
    {
      "epoch": 0.009761976498945465,
      "grad_norm": 15.142280578613281,
      "learning_rate": 4.983734056442704e-06,
      "loss": 2.0135,
      "step": 4050
    },
    {
      "epoch": 0.009786080144621874,
      "grad_norm": 27.544586181640625,
      "learning_rate": 4.98369388369991e-06,
      "loss": 2.2864,
      "step": 4060
    },
    {
      "epoch": 0.009810183790298282,
      "grad_norm": 23.992536544799805,
      "learning_rate": 4.983653710957116e-06,
      "loss": 2.3161,
      "step": 4070
    },
    {
      "epoch": 0.009834287435974691,
      "grad_norm": 24.651416778564453,
      "learning_rate": 4.983613538214322e-06,
      "loss": 2.1302,
      "step": 4080
    },
    {
      "epoch": 0.0098583910816511,
      "grad_norm": 19.07377815246582,
      "learning_rate": 4.983573365471528e-06,
      "loss": 2.6248,
      "step": 4090
    },
    {
      "epoch": 0.009882494727327508,
      "grad_norm": 23.81926155090332,
      "learning_rate": 4.983533192728734e-06,
      "loss": 2.1038,
      "step": 4100
    },
    {
      "epoch": 0.009906598373003917,
      "grad_norm": 14.874610900878906,
      "learning_rate": 4.9834930199859396e-06,
      "loss": 1.7064,
      "step": 4110
    },
    {
      "epoch": 0.009930702018680326,
      "grad_norm": 12.597908020019531,
      "learning_rate": 4.983452847243146e-06,
      "loss": 2.0536,
      "step": 4120
    },
    {
      "epoch": 0.009954805664356733,
      "grad_norm": 14.357572555541992,
      "learning_rate": 4.983412674500352e-06,
      "loss": 2.1756,
      "step": 4130
    },
    {
      "epoch": 0.009978909310033143,
      "grad_norm": 18.985260009765625,
      "learning_rate": 4.983372501757558e-06,
      "loss": 2.5135,
      "step": 4140
    },
    {
      "epoch": 0.010003012955709552,
      "grad_norm": 4.002882957458496,
      "learning_rate": 4.983332329014764e-06,
      "loss": 1.5867,
      "step": 4150
    },
    {
      "epoch": 0.01002711660138596,
      "grad_norm": 18.310054779052734,
      "learning_rate": 4.98329215627197e-06,
      "loss": 1.8489,
      "step": 4160
    },
    {
      "epoch": 0.010051220247062368,
      "grad_norm": 27.841402053833008,
      "learning_rate": 4.9832519835291756e-06,
      "loss": 2.2655,
      "step": 4170
    },
    {
      "epoch": 0.010075323892738776,
      "grad_norm": 34.67808532714844,
      "learning_rate": 4.983211810786382e-06,
      "loss": 1.8767,
      "step": 4180
    },
    {
      "epoch": 0.010099427538415185,
      "grad_norm": 9.051850318908691,
      "learning_rate": 4.983171638043588e-06,
      "loss": 1.7153,
      "step": 4190
    },
    {
      "epoch": 0.010123531184091594,
      "grad_norm": 11.055132865905762,
      "learning_rate": 4.983131465300794e-06,
      "loss": 2.924,
      "step": 4200
    }
  ],
  "logging_steps": 10,
  "max_steps": 1244625,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 275218720358400.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
